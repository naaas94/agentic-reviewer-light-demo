"""
Report generator for demo output.
Creates Markdown analysis reports from review results.
"""

from datetime import datetime
from typing import Any, Dict, List


class ReportGenerator:
    """Generates Markdown reports from review results."""

    def generate_report(
        self,
        results: List[Dict[str, Any]],
        run_id: str,
        metadata: Dict[str, Any]
    ) -> str:
        """Generate a complete Markdown report."""
        stats = self._calculate_stats(results)

        report = f"""---
title: Classification Audit Report
run_id: {run_id}
generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
samples_reviewed: {stats['total']}
accuracy: {stats['correct_pct']:.1f}%
---

# Classification Audit Report

## Executive Summary

This automated semantic audit reviewed **{stats['total']} text classification predictions**.
The analysis identified **{stats['incorrect']} predictions ({stats['incorrect_pct']:.1f}%)** requiring correction.

## Methodology

The agentic reviewer system evaluated each prediction by:
1. Semantically analyzing whether the predicted label fits the input text
2. Suggesting alternative labels for incorrect predictions
3. Generating natural language explanations for each decision

## Findings

### Verdict Distribution

| Verdict | Count | Percentage |
|---------|-------|------------|
| ✅ Correct | {stats['correct']} | {stats['correct_pct']:.1f}% |
| ❌ Incorrect | {stats['incorrect']} | {stats['incorrect_pct']:.1f}% |
| ❓ Uncertain | {stats['uncertain']} | {stats['uncertain_pct']:.1f}% |

### Label Distribution

| Label | Count |
|-------|-------|
{self._format_label_dist(stats['label_distribution'])}

## Sample Corrections

{self._format_corrections(stats['corrections'])}

## Recommendations

1. Review confusion patterns between semantically similar labels
2. Add training examples for commonly misclassified categories
3. Implement confidence thresholds for human review escalation

## Conclusion

The semantic audit demonstrates the system's capability to identify and correct
classification errors through LLM-powered semantic analysis.

---
*Generated by Agentic Reviewer Demo*
"""
        return report

    def _calculate_stats(self, results: List[Dict]) -> Dict[str, Any]:
        """Calculate statistics from results."""
        total = len(results)
        verdicts = {"Correct": 0, "Incorrect": 0, "Uncertain": 0}
        label_dist: Dict[str, int] = {}
        corrections: List[Dict[str, Any]] = []

        for r in results:
            verdict = r.get("verdict", "Uncertain")
            verdicts[verdict] = verdicts.get(verdict, 0) + 1

            label = r.get("pred_label", "Unknown")
            label_dist[label] = label_dist.get(label, 0) + 1

            if verdict == "Incorrect" and len(corrections) < 5:
                corrections.append({
                    "text": r.get("text", "")[:80],
                    "predicted": r.get("pred_label"),
                    "suggested": r.get("suggested_label"),
                    "reasoning": r.get("reasoning", "")[:100],
                })

        return {
            "total": total,
            "correct": verdicts["Correct"],
            "incorrect": verdicts["Incorrect"],
            "uncertain": verdicts["Uncertain"],
            "correct_pct": (verdicts["Correct"] / total * 100) if total else 0,
            "incorrect_pct": (verdicts["Incorrect"] / total * 100) if total else 0,
            "uncertain_pct": (verdicts["Uncertain"] / total * 100) if total else 0,
            "label_distribution": label_dist,
            "corrections": corrections,
        }

    def _format_label_dist(self, dist: Dict[str, int]) -> str:
        return "\n".join([f"| {label} | {count} |" for label, count in dist.items()])

    def _format_corrections(self, corrections: List[Dict]) -> str:
        if not corrections:
            return "*No corrections needed*"

        output = ""
        for i, c in enumerate(corrections, 1):
            output += f"""
**{i}. "{c['text']}..."**
- Predicted: `{c['predicted']}` → Suggested: `{c['suggested']}`
- Reason: {c['reasoning']}
"""
        return output

